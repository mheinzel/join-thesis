\section{Distributed Implementation}

We will now extend the implementation by adding the possibility to
create, refer to and migrate locations as in \distjoincalc.


\subsection{Migrating Actors}
\label{ch_migration}

Running Erlang processes cannot be migrated.
However, functions can be serialized, sent somewhere else and spawned there.
Since our provided primitives are the only places where actors are created,
we can equip them with the ability to ``wrap themselves up":
stopping their execution and returning
(a) a function capturing their behavior and
(b) their current state, which consists only of the values of their parameters.
For example, see the extended definition of the \code{forward} behavior
(figure \ref{distributed/wrap_up_forward}).
The returned data structure can then be sent to another node
and re-spawned there.

\thesislisting{distributed/wrap_up_forward}{the extended forward behavior}

As this creates a new process with a new \PID,
we need to generate our own globally unique names
that persist across migrations.
Fortunately, the Erlang ecosystem already provides global process registries
such as \emph{global}\footnote{\url{http://erlang.org/doc/man/global.html}},
which we just wrap in the \code{join_reg} module.
% TODO: gproc?

\TODO{3cm}{limitations with migrating processes}

Another problem we have to consider is that a migrating actor,
when it handles the \code{wrap_up} message and halts execution,
might still have messages in its inbox.
Worse, even after stopping execution,
nodes that are not yet aware of the migration
could try to send a message to the old \PID.
% TODO: and we don't want to globally lock
% TODO: problems with gproc?
To prevent the loss of messages, migrating actors will immediately unregister
from their name, wait until it becomes registered somewhere else
and stay alive for a limited time to forward any remaining messages
to the new process.


\subsection{Locations}

Locations are modelled as processes with registered location names,
arranged in a tree.
They keep track of their parent location, child locations and actors.
These references can be \PID{}s instead of names
to avoid the indirection of looking it up in the registry.
To see why \PID{}s suffice, note that actors and the parent location
cannot be migrated without the location itself.
When a child location migrates, it is not a child location anymore and can
simply be unregistered.
Similarly to actors, locations can be wrapped up,
unregistering themselves from their parent location and the registry,
and recursively wrapping up their child locations and actors.

Locations on the second level of the tree (directly below the root)
correspond to physical locations.
Each node has to initialize its top location in the beginning using
\code{join_location:create_root()}. These top locations cannot be moved.


\subsection{Primitives for Distribution}

We need to add an additional location parameter to many existing functions:
\begin{enumerate}[nosep]
  \item
    All actors need to be spawned at a specified location.
  \item
    While the current location is implicit in \distjoincalc,
    we choose to pass it around explicitly for now
    to simplify the implementation.
\end{enumerate}

Additionally, we implement two new constructs:
\code{def_location} and \code{go}.

\subsubsection{Location Definitions}
Since we restricted ourselves to implenting \corejoincalc,
we will only add location definitions with a binary join-pattern:

\begin{align*}
  \jcoreloc{a}{x}{u}{y}{v}{P}{R}{Q}
\end{align*}

Compared to the existing \code{def},
the corresponding Erlang function \code{def_location}
(figure \ref{distributed/def_location})
receives an additional location name parameter
(which is only used to generate better information for debugging).
The function in the existing parameter now also receives the new,
unique location name and returns three processes \code{\{P, R, Q\}}.
While \code{Q} is executed at the surrounding location,
\code{R} will be executed at the new location.

\thesislisting{distributed/def_location}{adapted location definition}

\thesislisting{distributed/def_location_usage}{using the location definition}

\subsubsection{Migration}

\begin{align*}
  \jgo{a}{\kappa}
\end{align*}

Again, we require to explicitly pass the current location
as an additional parameter.
The function \code{go(Location, Destination, Continuation)}
then only has to wrap up \code{Location} and send both
the resulting data structure and \code{Continuation}
to \code{Destination}.
There, the data will be re-spawned and the continuation called.

% TODO: create, wrap up etc. not exposed


\subsection{Connecting Nodes}

To send a message on a channel, we need to know its name.
But when we start separate nodes, they don't share any names,
so we need a way to exchange some names in the beginning.
On these, further names can be sent.

In \distjoincalc, we can define channels at the root of the location tree
(effectively without a location), but this is not realistic for the
implementation.
In \cite{fournet_jocaml:_2002},
a global name server is used to register and lookup names,
but in our implementation,
all names are registered in the process registry anyways.
This allows us to create globally known channels
in \code{def_globally} by using fixed Erlang atoms as keys.
(instead of unique, generated names).


\begin{example}[Applet]

  We define a server that provides an applet on the globally available name
  \code{cell} (figure \ref{distributed/example_applet_server}).
  When a message containing a location \code{A} and continuation
  is received on name \code{cell},
  a new location with names \code{Get} and \code{Put} will be defined (line 4)
  and sent to the continuation (line 15), where they can be used.
  Concurrently, the newly defined location with the cell migrates to location
  \code{A} (line 11).

  The client (figure \ref{distributed/example_applet_client})
  simply creates a continuation using the cell and sends its name and location
  to \code{cell} (line 16).

  \begin{figure}
    \begin{lstlisting}
rebar3 shell --sname node1
> node().
'node1@hostname'
> Root = join_location:create_root().
    \end{lstlisting}
    \begin{lstlisting}
rebar3 shell --sname node2
> net_kernel:connect_node('node1@hostname').
true
> Root = join_location:create_root().
    \end{lstlisting}
  \caption{connecting multiple Erlang shells}
  \label{connecting_shells}
  \end{figure}

  \thesislisting{distributed/example_applet_server}{the applet server}

  \thesislisting{distributed/example_applet_client}{the applet client}

\end{example}

To make the example work, the Erlang runtimes of the two nodes need to be
connected. In the shell, this can be achieved as shown in figure
\ref{connecting_shells}.
Also, they each need to create a root location
which can then be passed to the respective definitions.
